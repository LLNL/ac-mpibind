#!/bin/bash
#############################################################
# Edgar A. Leon
# Lawrence Livermore National Laboratory
#
# DO NOT COPY, MODIFY, OR DISTRIBUTE WITHOUT EXPLICIT
# PERMISSION FROM THE AUTHOR. ALL RIGHTS RESERVED. 
#
# This is version 12 of mpibind. 
# 
# Salient changes in this version:
#
# 1. Optimize task placement based on the hardware 
#    assignments of GPUs to NUMA domains.
#    To enable this feature use the MPIBIND option 'g'
#    At a high-level only those NUMA domains that have
#    GPUs are used for task placement. Other CPU resources
#    can be used through the utility threads framework,
#    e.g., UTC_NODE_LOCAL. 
# 2. Minor improvements for handling the prolog mode. 
# 3. Adding support to recognize the FLUX environment. 
#
#############################################################



# Created 'distribute' function to fix imbalanced GPU assignments. 
# I now treat GPU domains like I treat NUMA domains. 
# Once I set the GPU domains, I evenly divide the 
# tasks between the GPU domains. 
# With a separate program, I can check which GPU I am running on
# and how kernels are assigned to GPUs if CUDA_VISIBLE_DEVICES 
# is assigned more than one GPU. 
#
# Distribute tasks over elements with the goal of 
# not breaking domain boundaries 
# Input:
# - Flat array of elements 
# - Groups/Domains associative array 
# - Number of workers
# Output: 
# - Global array mapping workers to elements (pus):
#   worker_to_elems
#
# In the future, I could change the behavior of 'distribute' 
# when number of tasks is less than number of groups:
# Shall I assign one task per group (tasks stays within a GPU)? 
# (as opposed to providing all pus available)
# But then I could trimming the resources significantly!
# Currently, all pus available are assigned to the tasks. 
# It will fall under linear assignment and boundary crossing. 
#
# Notes when passing arrays by name:
# ${var#*=}: Strip out shortest match from beginning to '='.
# Another way to pass arrays without copying: 
# indices=$(eval echo "\${!$arr_name[@]}")    
# Less convenient because 'eval' is needed to query the array. 
# String and array subsets:
# ${stringZ:7:3} 
# "${A[@]:1:2}"
distribute()
{
    local tmp nw=$3 
    tmp="$(declare -p $1)"
    eval "declare -a array=${tmp#*=}"
    tmp="$(declare -p $2)"
    eval "declare -A group=${tmp#*=}"
    local x num_grps num_elems elems_per_worker wks_per_grp start str
    declare -a grp_array

    num_grps=${#group[@]}
    num_elems=${#array[@]}
    elems_per_worker=$(( num_elems / nw ))
    wks_per_grp=$(( nw / num_grps ))

    # Will mapping cross domain boundries? 
    if [ $(( nw % num_grps )) -ne 0 ]; then 
	# Linear assignment - boundary crossing 
	start=0
	for ((x=0; x<nw; x++)); do 
	    worker_to_elems[$x]="${array[@]:$start:$elems_per_worker}"
	    start=$(( start + elems_per_worker ))
	done
	# Commenting out this warning because all PUs within 
	# this NUMA domain can access all GPUs with the same latency.
	# I may need to revisit this if certain PUs are closer to 
	# one GPU than another within a NUMA domain. 
	# The 'assign_gpus' function would also have to change
	# to take this into account. 
	# if [ -n "$arg_warnings" ]; then 
	#     str="$myname: rank $rank spans multiple domains: "
	#     str+="${worker_to_elems[$wks_per_grp]}"
	#     single_node_print $str
	# fi
    else
	# Group balanced assignment 
	for ((grp=0; grp<num_grps; grp++)); do 
	    grp_array=( ${group[$grp]} ) 
	    start=0
	    for ((x=0; x<wks_per_grp; x++)); do 
		wk=$(( grp * wks_per_grp + x ))
		worker_to_elems[$wk]="${grp_array[@]:$start:$elems_per_worker}"
		start=$(( start + elems_per_worker ))
	    done
	done
    fi

    #declare -p worker_to_elems
}

# Test distribute
# all=( 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14)
#all=( 0 8 16 24 32 40 48 56 64 72 )
# declare -A group1=(
#     [0]="0 1 2 3 4"
#     [1]="5 6 7 8 9"
#     [2]="10 11 12 13 14"
# )
#declare -A group2=(
#    [0]="0 8 16 24 32"
#    [1]="40 48 56 64 72"
#)
#
#distribute all group1 3
#distribute all group2 4
#
#exit 


terminate()
{   
    if [ -n "$arg_sprolog" ]; then
	comm_print "Cancelling job step ${SLURM_JOBID}.${SLURM_STEPID}"
	if [ $rank -eq 0 ]; then
	    #kill -9 ${SLURM_TASK_PID}
	    scancel ${SLURM_JOBID}.${SLURM_STEPID}
	fi
    else
	exit $1 
    fi
}

comm_print()
{
    local str
    
    if [ -n "$arg_sprolog" ]; then
	# Print within a SLURM prolog.
	str=print
    fi
    
    if [ $rank -eq 0 ]; then
	echo -e $str "$myname: $@"
    fi
}

node_print()
{
    if [ -n "$arg_sprolog" ]; then
	# Print within a SLURM prolog.
	local str=print
    fi
    
    if [ $local_rank -eq 0 ]; then
	echo $str "$myname: $@"
    fi
}

single_node_print()
{
    local str firstnode
    
    if [ -n "$arg_sprolog" ]; then
	# Print within a SLURM prolog.
	str=print
    fi
    
    if [[ $SLURM_STEP_NODELIST =~ ([^0-9\[]+)\[?([0-9]+) ]]; then 
	firstnode="${BASH_REMATCH[1]}${BASH_REMATCH[2]}"
	if [[ $(hostname) =~ $firstnode ]]; then    
	    echo $str "$myname: $@"
	fi
    else
	# Not in a SLURM environment 
	if [ $rank -lt $local_size ]; then
 	    echo $str "$myname: $@"
	fi
    fi 
}

warn_print()
{
    if [ -n "$arg_warnings" ]; then 
	comm_print "Warn: $@"
    fi
}


# Cannot simply pass the array using 'array[@]'  
# because empty fields may be stripped. 
# Thus, passing the actual name of the array as parameter 
# and an optional size
disp_array()
{
    local arr_name=$1
    local i str indices

    indices=$(eval echo "\${!$arr_name[@]}")
    
    for i in $indices; do 
	str+="[$i]=\${$arr_name[$i]} "
    done
    eval echo "$str"
}


usage()
{
    if [ -z "arg_sprolog" ]; then 
	comm_print "Usage: srun|mpirun|jsrun [run_options] $myname <executable> [args]\n"
	comm_print "  Example: srun -N2 -n32 $myname lulesh 45\n"
    fi
    comm_print "  Options are specified via the environment variable MPIBIND\n"
    comm_print "  and separated by '.', e.g., MPIBIND=v.nw\n"
    comm_print "          v: show mapping\n"
    comm_print "          c: show command\n"
    comm_print "          u: show utility threads\n"
    comm_print "          g: optimize placement for GPU-intensive workloads\n"
    comm_print "         nw: omit warnings\n"
    comm_print "        hbm: use high-bandwidth memory first\n"
    #comm_print "    i<vars>: ignore specified environment variable(s), e.g., iCUDA...\n"
    
    terminate 0
}

parse_mpibind()
{
    local arg1 arg2 tmp field ARGS="$MPIBIND"
    #local hex="0-9a-fA-F"

    # Use MPIBIND_SPROLOG to run mpibind as a SLURM task prolog
    if [ -n "$MPIBIND_SPROLOG" ]; then
	if [[ $SLURM_TASK_PID =~ ^[0-9]+$ && $SLURM_JOBID =~ ^[0-9]+$ 
		    && $SLURM_STEPID =~ ^[0-9]+$ ]]; then 
	    arg_sprolog=y
	else
	    if [ $rank -eq 0 ]; then
		echo "print $myname: Cannot get SLURM_TASK_PID|SLURM_JOBID|SLURM_STEPID"
	    fi
	fi
    fi

    # Use MPIBIND_TOPOFILE to get the hardware topology
    if [[ $MPIBIND_TOPOFILE =~ ([^\ ]+) ]]; then 
	tmp=${BASH_REMATCH[1]}
	if [[ -e $tmp && -s $tmp && -f $tmp ]]; then 
	    topo_file=$tmp
	    #comm_print "topo_file=$topo_file"
	else
	    comm_print "Cannot open topology file $tmp"
	fi
    fi

    # Use MPIBIND_UTC_IN to indicate cpus for utility threads
    # p = pooled
    # d = distributed
    # c = calculated 
    if [[ $MPIBIND_UTC_IN =~ ([pdc]*)([0-9,-]+)? ]]; then
	#echo "1=${BASH_REMATCH[1]} 2=${BASH_REMATCH[2]}"
	arg1=${BASH_REMATCH[1]}
	arg2=${BASH_REMATCH[2]}
	if [[ $arg1 =~ c ]]; then 
	    arg_ut_calc=y 
	fi
	# pooled and distributed are mutually exclusive
	if [[ $arg1 =~ p ]]; then 
	    arg_ut_pooled=y
	elif [[ $arg1 =~ d ]]; then 
	    arg_ut_dist=y
	fi
	if [ -n "$arg2" ]; then 
	    # Utility threads' CPU resources
	    arg_ut_range=$arg2
	fi
	#echo "p=$arg_ut_pooled d=$arg_ut_dist c=$arg_ut_calc r=$arg_ut_range"
    fi

    # Use the MPIBIND variable to pass options
    # Parse strings separated by dots
    while [[ $ARGS =~ ([^\.]+)(.*) ]]; do 
	field=${BASH_REMATCH[1]};
	ARGS=${BASH_REMATCH[2]};
        #echo "field=$field rest=$ARGS"

	if [[ $field =~ vv ]]; then 
	    arg_verbose="vv"
	elif [[ $field =~ v ]]; then 
	    arg_verbose="v"
	elif [[ $field =~ c ]]; then 
	    arg_disp_cmd=y
	elif [[ $field =~ u ]]; then 
	    arg_disp_ut=y
	elif [[ $field =~ dd ]]; then 
	    arg_debug=dd
	elif [[ $field =~ d ]]; then 
	    arg_debug=d
	elif [[ $field =~ i(.+) ]]; then 
	    # Ignore specified environment vars
	    arg_ignore_vars+="${BASH_REMATCH[1]}"
	elif [[ $field =~ nw ]]; then 
	    # Do not display warnings
	    arg_warnings=""
	elif [[ $field =~ ^[0-9,-]+$ ]]; then 
	    arg_range=$(ints2bitmask $(parse_range $field))
	elif [[ $field =~ hbm ]]; then 
	    arg_hbm=1
	elif [[ $field =~ ^g ]]; then
	    arg_gpu_optimized=y
	elif [[ $field =~ j ]]; then 
	    # Ignore lrun options
	    /bin/true
	else
	    comm_print "  Unrecognized option '$field'"
	    usage
	fi
    done 
}



parse_range()
{
    local args="$@"
    local range subrange first last nums=""

    if [[ ! $args =~ ^[0-9,-]+$ ]]; then 
	single_node_print "Error: $args is not a valid cpu range" >&2
	terminate 0
    fi

    for range in $(echo $args | tr ',' ' '); do 
        if [[ $range =~ ^[0-9]+$ ]]; then 
	    nums+="$range "
        elif [[ $range =~ ^([0-9]+)-([0-9]+)$ ]]; then 
	    first=${BASH_REMATCH[1]}
	    last=${BASH_REMATCH[2]}
	    for i in $(seq $first $last); do 
		nums+="$i "
            done
        else
            single_node_print "Error: cannot parse '$range'" >&2
            terminate 0
        fi
    done

    echo $nums
}


# Input: 
#   Name of environment variable
# Output: 
#   If input variable is defined and in the ignore list, 
#   then return the empty string. Otherwise return 1, 
#   which indicates that variables should be set by mpibind
# defined ignore_list set
#       0           0   1
#       0           1   1
#       1           0   0
#       1           1   1
enforce_affinity_var()
{
    local var=$1
    
    if [[ -z ${!var} || $arg_ignore_vars =~ $var ]]; then
	echo 1
    fi
}


ints2bitmask()
{
    local array="$@" 
    local pos mask=0

    for i in $array; do 
	pos=$(( 1 << i ))
	mask=$(( mask | pos ))
    done

    printf "0x%x" $mask
}



# Fills in the following variables: 
# local_rank // node-local
# local_size // node-local
# rank // global 
# size // global 
# num_threads // task-local
mpi_env()
{
    ## Get my local and global ID info
    ## Use the appropriate variables for srun/mpirun/jsrun

    local correct str

    ## FLUX
    # Global: The total number of tasks to be run in the current job
    if [ -n "$FLUX_JOB_SIZE" ]; then
        size=$FLUX_JOB_SIZE
    fi
    # Comma separated list of task ranks run on the current node
    if [ -n "$FLUX_LOCAL_RANKS" ]; then
        local_size=$(echo $FLUX_LOCAL_RANKS | tr ',' ' ' | wc -w)
    fi
    # The relative task rank of the current task
    if [ -n "$FLUX_TASK_RANK" ]; then
        rank=$FLUX_TASK_RANK
    fi
    # The relative task rank of the current task on the current node
    if [ -n "$FLUX_TASK_LOCAL_ID" ]; then
        local_rank=$FLUX_TASK_LOCAL_ID
    fi

    ## CORAL                                                                   
    # Global: Number of processes in the job                              
    if [ -n "$JSM_NAMESPACE_SIZE" ]; then
        size=$JSM_NAMESPACE_SIZE
    fi
    # Node local: Number of local processes in the job    
    if [ -n "$JSM_NAMESPACE_LOCAL_SIZE" ]; then
        local_size=$JSM_NAMESPACE_LOCAL_SIZE
    fi
    # Global rank                                                   
    if [ -n "$JSM_NAMESPACE_RANK" ]; then
        rank=$JSM_NAMESPACE_RANK
    fi
    # Local rank                                                     
    if [ -n "$JSM_NAMESPACE_LOCAL_RANK" ]; then
        local_rank=$JSM_NAMESPACE_LOCAL_RANK
    fi

    ## Temporary work-around             
    # if [ -n "$PMIX_RANK" ]; then                                      
    #   rank=$PMIX_RANK                                                   
    #   local_rank=$(( rank % local_size ))                              
    # fi                                                                 
    ## For Intel MPI (Xeon Phi) I need PMI_RANK/SIZE                      
    ## if [[ $I_MPI_INFO_BRAND =~ Xeon\ +Phi ]]; then                     
    # if [ -n "$PMI_RANK" ]; then                                        
    #   local_rank=$PMI_RANK                                              
    #   rank=$PMI_RANK                                                    
    # fi                                                                  
    # if [ -n "$PMI_SIZE" ]; then                                          
    #   local_size=$PMI_SIZE                                                
    #   size=$PMI_SIZE                                                       
    # fi                                    

    if [ -n "$OMPI_COMM_WORLD_LOCAL_RANK" ]; then 
	local_rank=$OMPI_COMM_WORLD_LOCAL_RANK
    elif [ -n "$SLURM_LOCALID" ]; then 
	local_rank=$SLURM_LOCALID
    fi
    if [ -n "$OMPI_COMM_WORLD_LOCAL_SIZE" ]; then 
	local_size=$OMPI_COMM_WORLD_LOCAL_SIZE
    elif [ -n "$SLURM_TASKS_PER_NODE" ]; then 
	local_size=$(expr match "$SLURM_TASKS_PER_NODE" '\([0-9]*\)')
    fi
    if [ -n "$OMPI_COMM_WORLD_RANK" ]; then 
	rank=$OMPI_COMM_WORLD_RANK
    elif [ -n "$SLURM_PROCID" ]; then 
	rank=$SLURM_PROCID
        #rank=$MPIRUN_RANK
    fi
    if [ -n "$OMPI_COMM_WORLD_SIZE" ]; then 
	size=$OMPI_COMM_WORLD_SIZE
    elif [ -n "$SLURM_NPROCS" ]; then 
	size=$SLURM_NPROCS
        #size=$MPIRUN_NPROCS
    fi

    if [[ -z $local_rank || -z $local_size || -z $rank || -z $size ]]; then
	echo "$myname: Could not get MPI info from the environment"
	echo "   Try running $myname with lrun/srun/mpirun/jsrun"
	exit 0
    fi

    if [ -n "$OMP_NUM_THREADS" ]; then 
	#num_threads=$OMP_NUM_THREADS
	correct=0
	if [[ $OMP_NUM_THREADS =~ ^([0-9]+)$ ]]; then 
	    num_threads=${BASH_REMATCH[1]}
	    if [ $num_threads -gt 0 ]; then 
		correct=1
	    fi
	fi
	if [ $correct -eq 0 ]; then 
	    comm_print "Error: invalid OMP_NUM_THREADS=$OMP_NUM_THREADS"
	    exit 0
	fi
    fi
}

# If input command is successful return 0
# Otherwise return 1
test_cmd()
{
    type $@ > /dev/null 2>&1
    echo $?
}

# Evaluate within a SLURM prolog.  
# To export environment variables to the application,
# print out the export command.  
sp_eval()
{
    if [ -n "$arg_sprolog" ]; then
	# Print expression 
	echo $@
    else
	# Evaluate expression
	# Quote assigned values to handle spaces. 
	if [[ $@ =~ (.+)=(.+) ]]; then
     	    ${BASH_REMATCH[1]}="${BASH_REMATCH[2]}"
	else
	    $@
	fi
    fi
}


# Global vars:
#   'env_vars' associate array 
export_env_vars()
{
    local x
    
    for x in ${!env_vars[@]}; do
	if [ -n "$arg_sprolog" ]; then
	    echo export $x=${env_vars[$x]}
	else
	    #echo "[${env_vars[$x]}]"
	    export $x="${env_vars[$x]}"
	fi
    done
}

get_env_vars()
{
    local x str
    
    for x in ${!env_vars[@]}; do
	str+="$x='${env_vars[$x]}' "
    done

    echo $str
}


# Look for and set the topology command 
# Set global var 'topo_cmd'
set_topo_cmd()
{
    local rc
    
    if [ $(test_cmd lstopo-no-graphics) -eq 0 ]; then
	topo_cmd=lstopo-no-graphics
    elif [ $(test_cmd lstopo) -eq 0 ]; then 
	topo_cmd="lstopo -"	
    fi

    if [ -n "$topo_cmd" ]; then 
	if [ -n "$topo_file" ]; then 
	    #cmd+="--input knl-snc4-cache.xml"
	    #cmd+="--input knl-quad-flat.xml"
	    topo_cmd+=" --input $topo_file"
	fi
	#comm_print "topo_cmd=$topo_cmd"
	return 
    fi

    # type lstopo-no-graphics > /dev/null 2>&1 
    # rc=$?
    # if [ $rc -eq 0 ]; then 
    # 	topo_cmd=lstopo-no-graphics
    # 	return 
    # fi 

    # type lstopo > /dev/null 2>&1 
    # rc=$?
    # if [ $rc -eq 0 ]; then 
    # 	topo_cmd="lstopo -"
    # 	return 
    # fi 
    
    # summitdev does not have lstopo on compute nodes
    if [[ $(hostname) =~ summitdev ]]; then 
	type topo-cmd > /dev/null 2>&1 
	rc=$?
	if [ $rc -eq 0 ]; then 
	    topo_cmd=topo-cmd
	    return 
	fi 
    fi
    
    comm_print "$myname: Topology command not found"
    terminate 0
}


# Fetch the hardware topology
# Input parameters: levels array
#   levels: name of variable to write the level ids
#   array: name of associative array to write the pu ids of each level
topology()
{
    declare -A counted 
    local line token id pu x cmd range="" levels=""
    local smt pu_size core_size smt_levels i arr
    local levels_name=$1 hash_name=$2 name=$3 cpu_mask=$4

    if [ -n "$cpu_mask" ]; then 
	range="--restrict $cpu_mask"
    fi
  
    # 'merge' can collapse numa domains so using 
    # 'no-useless-caches' instead. At least one numa 
    # domain must be present. 
    cmd="$topo_cmd -p --no-useless-caches --no-io $range "
    #cmd="$topo_cmd -p --merge --no-io $range "
    #comm_print "$cmd" >&2
    # if [ $rank -eq 0 ]; then 
    #  	eval "$cmd" | head >&2 
    # fi

    while read line; do
	#echo "$line"
	
	while read token; do 
	    #echo "$token"
	    # There's only one Machine, thus ignore. 
	    if [[ $token =~ Machine ]]; then
		continue
	    fi
	    
	    # Get the name of the level 
	    if [[ $token =~ ^([a-zA-Z]+[0-9]*) ]]; then 
		id=${BASH_REMATCH[1]}
		#echo "id=$id"
		counted[$id]=0
		
		# Store the name of each level
		if [[ ! $levels =~ $id ]]; then 
		    levels+="$id "
		fi 
	    
		# Associate a PU physical id to respective levels
		# Levels with no PUs, e.g., KNL MCDRAM, are ignored. 
		# MCDRAM is handled as a device. 
		if [[ $token =~ PU\ P#([0-9]+) ]]; then 
		    pu=${BASH_REMATCH[1]}
		    for x in $levels; do 
			if [ ${counted[$x]} -eq 0 ]; then 
			    #pu_array[$x]+="$pu "
			    eval "$hash_name[$x]+=\"$pu \""
			    counted[$x]=1
			fi
		    done
		fi 
	    fi
	    
	    # A line may have multiple tokens separated by '+'
	    # e.g., Group0(Cluster) + NUMANode P#0 (24GB).
	    # Process each token separately. 
	    # In previous versions of mpibind, I collapsed 
	    # levels in the same line. But restricted topologies 
	    # may lead to non-uniform hierarchies and not keeping 
	    # all the levels may create issues. For example, 
	    # only 3 numa nodes available instead of 4 but 
	    # cluster0 shows 4. 
	done <<< "${line//+/$'\n'}"

	# Get the topology
    done <<<  "$(eval $cmd)"


    #####################
    ## Add SMT-levels 
    #####################
    if [[ $levels =~ Core.+PU ]]; then 
	# The system's SMT mode 
	pu_size=$(eval echo \${$hash_name[PU]} | wc -w)
	core_size=$(eval echo \${$hash_name[Core]} | wc -w)
	smt=$(( pu_size / core_size ))
	
	smt_levels=""
	for ((i=2; i<smt; i++)); do 
	    id="SMT-$i"
	    arr=$(create_smt_pus $i $hash_name)
	    #echo "smt_arr=$arr"
	    eval "$hash_name[$id]=\"$arr\""
	    smt_levels+="$id "
	done
	
	#echo "$smt_levels"
	if [ -n "$smt_levels" ]; then 
	    levels="${levels/Core /Core $smt_levels}"
	fi
    fi


    #echo "levels: $levels"
    #declare -p $hash_name
    if [ "$name" != GT ]; then 
	if [[ $arg_verbose =~ "vv" ]]; then 
	    comm_print "$myname:$name Topology: $levels" #>&2
	    for x in $(eval echo \${!$hash_name[@]}); do 
		comm_print "$myname:$name $x: $(eval echo \${$hash_name[$x]})" #>&2
	    done
	fi
    fi

    # Copy levels to variable passed as parameter
    eval "$levels_name=\"$levels\""
}


# Example: 
# Input: 
#  markers=0 8
#  elems=0 1 2 5 7 8 9 10
# Output: 
#  out_array=( "0 1 2 5 7" "8 9 10" )
get_subsets()
{
    declare -a markers=("${!1}") 
    declare -a elems=("${!2}")
    local out_array_name=$3
    local x index mark first last_elem
    
    index=0
    mark=${markers[$index]}
    first=1
    last_elem=${elems[${#elems[@]}-1]}

    for x in ${elems[@]}; do
 
	# Boundary 
	if [ $x -eq $mark ]; then
	    if [ $first -ne 1 ]; then
		# Done with subset
		# comm_print "$index: subset=$subset"
		eval "$out_array_name[index-1]+=\"$subset\""
	    fi
	    first=0
	    subset=""
	    index=$(( index + 1 ))
	    # Avoid out-of-bounds access
	    if [ $index -lt ${#markers[@]} ]; then 
		mark=${markers[$index]}
	    fi
	fi
	
	# Flush on last iteration 
	if [ $x -eq $last_elem ]; then 
	    subset+="$x "
	    # Done with subset
	    # comm_print "$index: subset=$subset"
	    eval "$out_array_name[index-1]+=\"$subset\""
	fi

	subset+="$x "
    done 
}


# Break a string into substrings whose starting 
# elements are given by the markers 
# In: 
#  markers="33 7 12 15 18"
#  str="5 76 7 78 9 10 12 15"
# Out: 
#  [0]=5 76 [1]=7 78 9 10 [2]=12 [3]=15
split_string()
{
    declare -a markers=("${!1}") 
    local str="$2" out_arr_name=$3
    local i count 
    #echo "input: =$1= =$2= =$3="
    
    count=0
    for ((i=0; i<${#markers[@]}; i++)); do 
	#echo "marker=${markers[i]}"
	if [[ $str =~ (^|.*\ )(${markers[i]}(\ .*|$)) ]]; then 
	    #echo "head=${BASH_REMATCH[1]} tail=${BASH_REMATCH[2]}"
	    str="${BASH_REMATCH[2]}"
	    if [ -n "${BASH_REMATCH[1]}" ]; then 
		eval "$out_arr_name[count]=\"${BASH_REMATCH[1]}\""
		count=$(( count + 1 ))
	    fi
	fi
    done

    # Get the last piece
    #echo "rem: $str"
    eval "$out_arr_name[count]=\"$str\""
    #echo "$out_arr_name[count]=\"$str\""
}


#markers=(0 8)
#str="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15"
#markers=(33 7 12 15 18)
#str="5 76 7 78 9 10 12 15"

#split_string markers[@] "$str" hola
#disp_array hola
#exit


# Reorder input elements by set
# Example:
# Input:
#  array=( "5 6" "7 8 9 10" "12 15" )
# Output: 
#  out="5 7 12 6 8 15 9 10"
set_reorder()
{
    declare -a array=("${!1}") 
    local i j indices max_set_size str out size

    # Find out the number of elements of the largest set 
    max_set_size=0
    for ((i=0; i<${#array[@]}; i++)); do 
	size=$(echo "${array[i]}" | wc -w)
	#echo "size=$size"
	if [ $size -gt $max_set_size ]; then 
	    max_set_size=$size
	fi
    done

    # Iters: number of elements of the max set
    for ((j=0; j<max_set_size; j++)); do 
	# For each set, pick up the first element 
	for ((i=0; i<${#array[@]}; i++)); do 
	    if [[ ${array[i]} =~ ([0-9]+)(.*) ]]; then 
		out+="${BASH_REMATCH[1]} "
		array[i]="${BASH_REMATCH[2]}"
	    fi
	done
    done

    echo "$out"
}



## Input: 
## - size: number of elements
## - an array of strings, each string (set) comprised of integers
## From each set get the first x elements so that 'size' elements 
## are selected as evenly as possible from all the input sets.   
set_select()
{
    local size=$1
    declare -a orig=("${!2}")
    declare -a sets=("${!2}")
    declare -a counts=()
    local arr str out

    ## Initialize counts per set 
    for ((arr=0; arr<${#sets[@]}; arr++)); do 
	    counts[$arr]=0
    done

    ## Count how many elements per set are needed
    while [ $size -gt 0 ]; do 
	# For each set, pick up the first element 
	for ((arr=0; arr<${#sets[@]}; arr++)); do 
            ## Keep track of how many elements per set 
	    if [[ ${sets[arr]} =~ ([0-9]+)(.*) ]]; then 
		counts[$arr]=$(( ${counts[$arr]} + 1 ))
		sets[arr]="${BASH_REMATCH[2]}"
		size=$(( size - 1 ))
		if [ $size -le 0 ]; then 
		    break 
		fi 
	    fi
	done
    done

    ## Select 'counts[x]' elements per set
    for ((arr=0; arr<${#orig[@]}; arr++)); do 
	size=${counts[$arr]}
	str=( ${orig[$arr]} )
	#echo "counts[$arr]=$size"
	#echo "orig[$arr]=${orig[$arr]}"
	out+="${str[@]:0:$size} "
    done

    echo "$out"
}
#a[0]="0 1 2 3 4" 
#a[3]="5 8 9 10"
#a[5]="11 12"
#set_select 9 a[@]
#exit


# Discover the NIC and GPU devices
# Assumes that GPUs belong to a NUMA domain. 
# NVIDIA GPUs have the following identifier: '10de:'
# (e.g, PCI 10de:102d) but there is not distinct id 
# when having more than one in a node. 
# Assumes global variables: 
#  Associative arrays: gpus_per_numa, nics_per_numa
#  Scalars: num_gpus, num_nics
# Fills in the above global variables, e.g., 
#   gpus[numanode]=gpu_ids: 
#   0->"0 1", 1->"2 3"
discover_devices()
{
    local line cmd gpu=-1 numanode=-1 

    # Do not restrict the topology (--restrict) here because
    # doing so may result in not showing MCDRAM memories. 
    cmd="$topo_cmd -p | grep -E -e NUMA "
    # NVIDIA and AMD GPUs
    cmd+="-e 10de: -e 1002: "
    # Network 
    cmd+="-e Net.+ib -e Net.+hsi"
    #comm_print "$cmd"

    while read line; do 
	#echo "$line"
	if [[ $line =~ NUMANode ]]; then 
	    if [[ $line =~ MCDRAM.+P#([0-9]+) ]]; then
		mcds_per_numa[$numanode]+="${BASH_REMATCH[1]} "
		num_mcds=$(( num_mcds + 1 ))
	    else
		numanode=$(( numanode + 1 ))
	    fi
	elif [[ $line =~ 10de|1002 ]]; then 
	    gpu=$(( gpu + 1 ))
	    gpus_per_numa[$numanode]+="$gpu "
	    num_gpus=$(( num_gpus + 1 )) 
	elif [[ $line =~ Net.+(ib|hsi)([0-9]+) ]]; then 
	    nics_per_numa[$numanode]+="${BASH_REMATCH[2]} "
	    num_nics=$(( num_nics + 1 ))
	fi
    done <<< "$(eval $cmd)"

    if [[ $arg_verbose =~ "vv" ]]; then 
	# comm_print "$myname: Topology: $topo_levels"
	# for x in ${!pu_array[@]}; do 
	# 	comm_print "$myname: $x: ${pu_array[$x]}"
	# done
	if [ $num_mcds -ge 1 ]; then
	    comm_print "$myname: MCDs: ${mcds_per_numa[@]}"
	fi
	if [ $num_gpus -ge 1 ]; then 
	    comm_print "$myname: GPUs: ${gpus_per_numa[@]}"
	fi
	if [ $num_nics -ge 1 ]; then 
	    comm_print "$myname: NICs: ${nics_per_numa[@]}"
	fi
    fi
}


# For each PU assign a GPU(s) within a NUMA domain 
# When possible, spread the available GPUs over the 
# PUs, e.g., if there's two GPUs and 4 PUs, use 
# GPU1 for first two PUs and GPU2 for the other two PUs. 
# Input: 
# - gpus of numa domain // must be passed within quotation marks 
# - pus of numa domain
# Output: 
# - global hash 'gpus_per_pu'
# - global array 'pus_per_gpu' // clear value before entering function
assign_gpus()
{
    local num_pus num_gpus numa_pus numa_gpus
    local avepus extra start i x num_pus_per_gpu

    numa_gpus=( $1 )
    shift 
    numa_pus=( $@ )
    num_pus=${#numa_pus[@]}
    num_gpus=${#numa_gpus[@]}
    # comm_print "$(declare -p numa_gpus)"
    # comm_print "$(declare -p numa_pus)"
    # comm_print "num_pus=$num_pus num_gpus=$num_gpus"

    if [ $num_pus -ge $num_gpus ]; then 
        # Single GPU per PU assignment 
	avepus=$(( num_pus / num_gpus ))
	extra=$(( num_pus % num_gpus ))
	
	pus_per_gpu=()
	start=0
	for ((i=0; i<num_gpus; i++)); do 
	    if [ $i -lt $extra ]; then 
		num_pus_per_gpu=$(( avepus + 1 ))
	    else
		num_pus_per_gpu=$avepus
	    fi
	    pus_per_gpu[$i]="${numa_pus[@]:$start:$num_pus_per_gpu}"
	    #comm_print "pus_per_gpu[$i]: ${pus_per_gpu[$i]}"
	    start=$(( start + num_pus_per_gpu ))

	    # Assign GPU to each PU
	    for x in ${pus_per_gpu[$i]}; do 
		gpus_per_pu[$x]=${numa_gpus[$i]}
	    done
	done
    else
	# Multi-GPU per PU assignment 
	for x in ${numa_pus[@]}; do 
	    gpus_per_pu[$x]="${numa_gpus[@]}"
	done
    fi

    #echo "num_gpus=$num_gpus num_pus=$num_pus"
}

#Todo: account for other global variables
# I may need extra paramters for num_tasks per numa_grp! 
# Global variables:
#   gpus_per_numa 
distribute_gpus()
{
    local numa_grp=$1 gpu_set=$2 overwrite=$3
    local gpus num_gpus numas_no_gpus num_numas num_tasks 
    local i res my_gpu gpu_idx my_numa numa_id 

    
    gpus=( $gpu_set )
    num_gpus=${#gpus[@]}

    for i in $numa_grp; do
	if [ -z "${gpus_per_numa[$i]}" ]; then
	    numas_no_gpus+="$i "
	fi
	num_tasks=$(( num_tasks + ${tasks_per_numa[$i]} ))
    done
    #comm_print "num_tasks $num_tasks"
    if [ -n "$overwrite" ]; then
	numas_no_gpus="$numa_grp"
    fi
    
    numas_no_gpus=( $numas_no_gpus )
    num_numas=${#numas_no_gpus[@]}

    # Assign it: two cases.
    if [ $num_numas -ge $num_gpus ]; then
	# More NUMA nodes than GPUs: Distribute nodes among GPUs.
	mod=$(( num_tasks % num_numas ))
	if [ $mod -eq 0 ]; then 
	    # Balanced num. tasks over NUMA domains, thus
	    # distribute GPUs over NUMA domains evenly 
	    for ((i=0; i<num_numas; i++)); do 
		res=( $(map_to_domains $i $num_numas $num_gpus) )
		my_gpu=${res[0]}
		#echo "${res[@]}"
		numa_id=${numas_no_gpus[$i]}
		gpus_per_numa[$numa_id]=${gpus[$my_gpu]}
	    done
	else
	    # 4/5/2019: 
	    # Adjacent numa domains should get different GPUs:
	    #    numa0->GPU0 numa1->GPU1 numa2->GPU0 numa3->GPU1
	    # Otherwise if only two tasks are present (mapped to
	    # first and second NUMA domain) they map to the same GPU:
	    #    numa0->GPU0 numa1->GPU0 numa2->GPU1 numa3->GPU1
	    gpu_idx=0
	    for ((i=0; i<num_numas; i++)); do
		numa_id=${numas_no_gpus[$i]}
		gpus_per_numa[$numa_id]=${gpus[$gpu_idx]}
		gpu_idx=$(( gpu_idx + 1 ))
		if [ $gpu_idx -ge $num_gpus ]; then
		    gpu_idx=0
		fi
	    done	
	fi
    else
	## Reverse assignment: Assign a NUMA node to each GPU	
	for ((i=0; i<num_gpus; i++)); do 
	    res=( $(map_to_domains $i $num_gpus $num_numas) )
	    my_numa=${res[0]}
	    #echo "${res[@]}"
	    numa_id=${numas_no_gpus[$my_numa]}
	    gpus_per_numa[$numa_id]+="${gpus[$i]} "
	done
    fi

    if [[ $arg_debug =~ dd ]]; then
	comm_print "dist_gpus: numa_grp $numa_grp; gpu_set $gpu_set" 
	comm_print "dist_gpus: gpus_per_numa $(disp_array gpus_per_numa)"
    fi
}


# Global variables:
#   gpus_per_numa
numa_with_largest_gpuset()
{
    local numa_grp="$@"
    local i numa_max_gpus numa_with_gpus gpus_arr

    numa_max_gpus=0
    for i in $numa_grp; do 
	if [ -n "${gpus_per_numa[$i]}" ]; then
	    # Find the NUMA domain with the most GPUs
	    gpus_arr=( ${gpus_per_numa[$i]} )
	    if [ ${#gpus_arr[@]} -gt $numa_max_gpus ]; then
		numa_with_gpus=$i
		numa_max_gpus=${#gpus_arr[@]}
	    fi
	fi
    done

    echo $numa_with_gpus
}


# Not all numa domains have associated GPUs
# Look for GPUs attached to the same group and assign those. 
# When there's no GPU in that group, assign all GPUs.  
# This function uses the following global vars:
#  topo_levels 
#  numa_array
#  pu_array
#  gpus_per_numa
assign_gpus_to_numadoms()
{
    # Distribute the GPUs. It includes architectures with GPUs {A} on
    # one group/package and GPUs {B} on another group/package and
    # some NUMA domains do not have GPUs associated with them.
    local grp_level numa_arr grp_arr grp_numas pu_to_numa
    local numas_no_gpus grp_gpus numa_grp all_gpus
    local i grp numa gpus
    
    # Save the node-wide gpu set
    all_gpus="${gpus_per_numa[@]}"
    #comm_print "gpus_per_numa $(disp_array gpus_per_numa)"
    
    numa_arr=( ${pu_array[NUMANode]} )
    # Get reverse mapping (PU to NUMA)
    # Needed to get the NUMA indices 
    for ((i=0; i<${#numa_arr[@]}; i++)); do
	pu_to_numa[${numa_arr[$i]}]=$i
    done
    #comm_print "pu_to_numa $(disp_array pu_to_numa)"
    
    if [[ $topo_levels =~ ([^\ ]+)\ +NUMA ]]; then
	# There's a level higher than NUMA
	grp_level=${BASH_REMATCH[1]}
	grp_arr=( ${pu_array[$grp_level]} )
    else
	# Avoid a special case when the top level is NUMA. 
	# If there's no groups/packages (top level is NUMA)
	# create a group with all NUMA nodes  
	grp_arr=( ${numa_arr[0]} )
    fi
    #comm_print "grp_arr $(disp_array grp_arr)"
    
    # Get the NUMA nodes associated with each group,
    # e.g., [0]="0 6 12 18"
    get_subsets grp_arr[@] numa_arr[@] "grp_numas"
    #comm_print "grp_numas $(disp_array grp_numas)"

    # For each group/package, 
    # get the gpus in the group and distribute them
    # over all of the numa nodes in the group
    # (overwriting existing gpu assignments)
    for ((grp=0; grp<${#grp_numas[@]}; grp++)); do
	numas_no_gpus=""
	grp_gpus=""
	numa_grp=""
	# Iterate over numa nodes in this group 
	for i in ${grp_numas[$grp]}; do
	    numa=${pu_to_numa[$i]}
	    numa_grp+="$numa "
	    gpus="${gpus_per_numa[$numa]}"
	    
	    if [ -z "$gpus" ]; then
		numas_no_gpus+="$numa "
	    else
		grp_gpus+="$gpus"
	    fi
	done
	#comm_print "numas_no_gpus $numas_no_gpus"
	#comm_print "numa_grp $numa_grp"
	#comm_print "grp_gpus $grp_gpus"

	# If a group does not have any GPUs at all,
	# use all of the available node-wide GPUs.
	# Another strategy would be to find the grp
	# with the largest num of gpus and use this set. 
	if [ -z "$grp_gpus" ]; then
	    grp_gpus="$all_gpus"
	fi
	
	# This group has numa nodes without gpus
	# Assign gpus to all numa nodes in this group
	# even to those nodes that have gpus (overwrite)
	if [ -n "numas_no_gpus" ]; then
	    distribute_gpus "$numa_grp" "$grp_gpus" overwrite
	fi
    done
    if [[ $arg_debug =~ dd ]]; then
	comm_print "assign_gpus_to_numadoms: gpus_per_numa $(disp_array gpus_per_numa)"
    fi


    ### Older version ###
    # Assign GPUs to NUMA domains that do not have GPUs
    # from the NUMA domain with the largest number of GPUs.
    # A side-effect of this assignment may be:
    # The NUMA domain with the largest number of GPUs
    # may endup with a single GPU after reassignment. 
    # If only one process is launched and the chosen NUMA is this
    # NUMA domain, then only one GPU may be assigned to this process
    # (instead of all of the GPUs of this NUMA domain).
    if [ ]; then 
	local numa_with_gpus existing_gpus numa_grp
	
	if [ ${#gpus_per_numa[@]} -lt $numa_nodes ]; then 
	    # Find GPUs
	    numa_grp="${!numa_array[@]}"
	    numa_with_gpus=$(numa_with_largest_gpuset $numa_grp)
	    existing_gpus="${gpus_per_numa[$numa_with_gpus]}"
	    
	    # The GPUs of the chosen NUMA domain will be reassinged.
	    gpus_per_numa[$numa_with_gpus]=""
	    
	    distribute_gpus "$numa_grp" "$existing_gpus"
	fi
    fi
}


# Reorder a sequence of ints by a given stride
# Input: 
# - stride 
# - string of numbers 
# Example: 
# stride=3 str=8 10 12 14 16 18 20 22 24
# out=8 14 20 10 16 22 12 18 24
stride_reorder()
{
    local stride array 
    local j out start
    
    stride=$1
    shift 
    array=( $@ )
    
    for ((start=0; start<stride; start++)); do 
	# Get all elements positioned at multiples of stride 
	for ((j=start; j<${#array[@]}; j++)); do 
	    if [ $(( (j-start) % stride )) -eq 0 ]; then 
		out+="${array[j]} "
	    fi
	done
    done
    echo "$out"
}



## Calculate the application and utility thread cpu masks. 
## These mask can be used as input to hwloc to restrict the
## topology. 
## Input: list of cpus for the utility threads 
## The masks may not be restricted appropriately when 
## MPIBIND_TOPOFILE is specified because '--restrict binding' 
## won't take effect and because the commands 'numactl' and 
## 'taskset' are executed on the real host. 
calc_cpu_masks()
{
    local ut_range=$1 out_app_mask=$2 out_ut_mask=$3
    local mask_cmd cmd app_mask ut_mask 
    
    if [ -n "$ut_range" ]; then 
	# Use hwloc to get the affinity cpu mask.
	# With this mask I can then restrict hwloc to a subset
	# of resources, e.g., the utility threads or app threads
	mask_cmd="$topo_cmd --only machine --cpuset-only "
	#--taskset
	mask_cmd+="--restrict binding"

	# Use numactl to get the complement of the utiliy
	# threads cpu range, i.e., the app cpus
	cmd="numactl --all --physcpubind=!${ut_range} $mask_cmd"
	#echo "$cmd"
	app_mask=$(eval $cmd)
	eval "$out_app_mask=$app_mask"
    
	# Get the utility threads mask 
	cmd="taskset --cpu-list $ut_range $mask_cmd"
	#echo "$cmd"
	ut_mask=$(eval $cmd)
	eval "$out_ut_mask=$ut_mask"
	
	#echo "$app_mask $ut_mask" 
    fi
}


# Map node-local processes/tasks to domains (e.g., NUMA)
# Input: my_rank/id
#        number of processes/tasks
#        number of domains
# Returns "my_dom my_np_within_dom my_id_within_dom"
map_to_domains()
{
    local rank=$1 np=$2 ndoms=$3
    local i cum_np np_per_dom_ave np_per_dom_extra tmp

    np_per_dom_ave=$(( np / ndoms ))
    np_per_dom_extra=$(( np % ndoms ))

    cum_np=0
    for ((i=0; i<ndoms; i++)); do 
	if [ $i -lt $np_per_dom_extra ]; then 
	    tmp=$(( np_per_dom_ave + 1 ))
	else
	    tmp=$np_per_dom_ave
	fi 
	prev=$cum_np
	cum_np=$(( cum_np + tmp ))
	if [ $rank -lt $cum_np ]; then
	    # my_dom=$i
	    # my_np_within_dom=$tmp
	    # my_id_within_dom=$(( rank % tmp ))
	    echo "$i $tmp $(( (rank-prev) % tmp ))"
	    break
	fi
    done
}

# The algorithm for this function *must* be the same
# as in 'map_to_domains,' except that this function
# returns a different aspect.
# Return the number of elements assigned to each domain
# Input:
#   np number of elements
#   ndoms number of domains
map2doms_elemsperdom()
{
    local np=$1 ndoms=$2 
    local i avg rem str

    avg=$(( np / ndoms ))
    rem=$(( np % ndoms ))

    for ((i=0; i<ndoms; i++)); do
	if [ $i -lt $rem ]; then
	    str+="$(( avg + 1 )) "
	else
	    str+="$avg "
	fi
    done

    echo "$str"
}



# Map a process/task within a domain context to PUs.
# Input: Process ID within domain
#        Number of processes within domain 
#        Array of PUs corresponding to the process' domain 
# Output: A string of PUs separated by spaces. 
map_to_pus()
{
    local my_id_within_dom=$1 my_np_within_dom=$2 my_pus_within_dom=$3
    local i offset nelems pu_array pu_size ave extra

    pu_array=( $my_pus_within_dom )
    pu_size=${#pu_array[@]}

    if [ $my_np_within_dom -le $pu_size ]; then 
	ave=$(( pu_size / my_np_within_dom ))
	extra=$(( pu_size % my_np_within_dom ))
	offset=0
	for ((i=0; i<my_np_within_dom; i++)); do
	    if [ $i -lt $extra ]; then 
		nelems=$(( ave + 1 ))
	    else
		nelems=$ave
	    fi
	    if [ $i -eq $my_id_within_dom ]; then 
		#printf "%02d: begin=$offset size=$nelems\n" $local_rank
		echo "${pu_array[@]:$offset:$nelems}"
		break
	    fi
	    offset=$(( offset + nelems ))
	done
    else
	# If resources are overcommitted
	# use a simple module assignment 
	offset=$(( my_id_within_dom % pu_size ))
	echo ${pu_array[$offset]}
    fi
}


# New num_threads assignment
# Motivating example: KNL/snc4 may have different 
# number of cores per NUMA domain. 
# For example, knl-snc4-cache, 68 cores, 4 NUMA nodes,
# 15 tasks per node: 
# 0: my_numa=0 my_np_per_numa=4 my_id=0 cores=18 nthreads_ave=4 
#    nthreads_extra=2 num_threads=5
# 1: my_numa=0 my_np_per_numa=4 my_id=1 cores=18 nthreads_ave=4 
#    nthreads_extra=2 num_threads=5
# 2: my_numa=0 my_np_per_numa=4 my_id=2 cores=18 nthreads_ave=4 
#    nthreads_extra=2 num_threads=4
# 3: my_numa=0 my_np_per_numa=4 my_id=3 cores=18 nthreads_ave=4 
#    nthreads_extra=2 num_threads=4
# 4: my_numa=1 my_np_per_numa=4 my_id=0 cores=18 nthreads_ave=4 
#    nthreads_extra=2 num_threads=5
# 5: my_numa=1 my_np_per_numa=4 my_id=1 cores=18 nthreads_ave=4 
#    nthreads_extra=2 num_threads=5
# 6: my_numa=1 my_np_per_numa=4 my_id=2 cores=18 nthreads_ave=4 
#    nthreads_extra=2 num_threads=4
# 7: my_numa=1 my_np_per_numa=4 my_id=3 cores=18 nthreads_ave=4 
#    nthreads_extra=2 num_threads=4
# 8: my_numa=2 my_np_per_numa=4 my_id=0 cores=16 nthreads_ave=4 
#    nthreads_extra=0 num_threads=4
# 9: my_numa=2 my_np_per_numa=4 my_id=1 cores=16 nthreads_ave=4 
#    nthreads_extra=0 num_threads=4
# 10: my_numa=2 my_np_per_numa=4 my_id=2 cores=16 nthreads_ave=4 
#     nthreads_extra=0 num_threads=4
# 11: my_numa=2 my_np_per_numa=4 my_id=3 cores=16 nthreads_ave=4 
#     nthreads_extra=0 num_threads=4
# 12: my_numa=3 my_np_per_numa=3 my_id=0 cores=16 nthreads_ave=5 
#     nthreads_extra=1 num_threads=6
# 13: my_numa=3 my_np_per_numa=3 my_id=1 cores=16 nthreads_ave=5 
#     nthreads_extra=1 num_threads=5
# 14: my_numa=3 my_np_per_numa=3 my_id=2 cores=16 nthreads_ave=5 
#     nthreads_extra=1 num_threads=5

# Calculate default number of threads if user did
# not specify one. 
# Not using this function anymore. Instead, use
# 'map_to_pus'
calc_num_threads()
{
    local my_id_within_numa=$1 my_numa_dom=$2 my_np_per_numa=$3
    local nthreads my_ncores_per_numa nthreads_ave nthreads_extra
    local core_array num_cores numa_cores

    core_array=( ${pu_array[Core]} )
    num_cores=${pu_size[Core]}
    get_subsets numa_array[@] core_array[@] "numa_cores"
    #declare -p numa_cores

    my_ncores_per_numa=$(echo ${numa_cores[$my_numa_dom]} | wc -w)
    nthreads_ave=$(( my_ncores_per_numa / my_np_per_numa ))
    nthreads_extra=$(( my_ncores_per_numa % my_np_per_numa ))
    if [ $my_id_within_numa -lt $nthreads_extra ]; then 
	nthreads=$(( nthreads_ave + 1 ))
    else
	nthreads=$nthreads_ave
    fi

    echo $nthreads 
}


## Function for the Utility threads work.
## Given a number of processes, assign PUs to each one
## based on a restricted topology represented by 'cpu_mask.' 
## Currently, this function does not find the "highest" level
## that would satisfy the number of processes. It always chooses
## the "lowest" level with all of the associated PUs. 
## Input: 
##   Implict: local_rank, local_size
##  Explicit: cpu_mask 
## Output: string of PUs separated by spaces. 
map2cpus()
{
    local cpu_mask=$1 out_name=$2
    declare -A lf_pu_array lf_pu_size
    local lf_topo_levels lf_numa_nodes lf_numa_array 
    local level res my_numa my_np_within_numa my_id_within_numa
    local level_pu_array numa_elems my_elems_within_numa str 
    
    # Get the node's topology 
    #echo "$cpu_mask"
    topology "lf_topo_levels" "lf_pu_array" UT $cpu_mask
    
    # Get the number of elements for each level of the topology
    for level in $lf_topo_levels; do 
	lf_pu_size[$level]=$(echo ${lf_pu_array[$level]} | wc -w)
    done 
    
    # NUMA domains 
    # Number of numa nodes and the 
    # PU identifiers where each domain start
    lf_numa_nodes=${lf_pu_size[NUMANode]}
    lf_numa_array=( ${lf_pu_array[NUMANode]} )
    #declare -p lf_pu_size
    #declare -p lf_numa_array

    # Map tasks to NUMA domains
    res=( $(map_to_domains $local_rank $local_size $lf_numa_nodes) )
    my_numa=${res[0]}
    my_np_within_numa=${res[1]}
    my_id_within_numa=${res[2]}

    # Get the PUs associated with each process. 
    # Assignment is done using the PU level because 
    # I do not know if this is the only level available 
    # since the cpu_mask may or may not contain any other level.
    # I could potentially check what levels are available and 
    # choose the one that satisifes the number of processes, but
    # this should be fine for now. 
    level_pu_array=( ${lf_pu_array[PU]} )
    get_subsets lf_numa_array[@] level_pu_array[@] "numa_elems"
    my_elems_within_numa="${numa_elems[$my_numa]}"
    res=( $(map_to_pus $my_id_within_numa $my_np_within_numa "$my_elems_within_numa") )
    #my_pus="${res[@]}"
    eval "$out_name='${res[@]}'"

    str="%02d: my_numa=$my_numa my_id_wnuma=$my_id_within_numa/"
    str+="$my_np_within_numa num_pus=${#res[@]} pus=${!out_name}"
    if [[ $arg_debug =~ d ]]; then 
	printf "$str\n" $local_rank #>&2
    fi 
    #echo "$my_pus"
}

# Given two strings of numbers str1 and str2 (separated by spaces), 
# find the numbers from str2 that are not in str1. 
# Using 'grep' instead of 'comm' because the latter would involve
# additional sort operations: 
#tmp1=$(comm -23 <(echo "$my_pus_all" | tr ' ' '\n' | sort) <(echo "$my_pus" | tr ' ' '\n' | sort) | sort -n | tr '\n' ' ')
#tmp1=$(comm -23 --nocheck-order <(echo "$my_pus_all" | tr ' ' '\n' | sort -n) <(echo "$my_pus" | tr ' ' '\n' | sort -n) | tr '\n' ' ')
set_complement()
{
    local str1=$1 str2=$2
    local cmd 

    cmd="echo \"$str2\" | tr ' ' '\n' "
    cmd+="|grep -vxF -f <(echo \"$str1\" | tr ' ' '\n') "
    #cmd+="|tr '\n' ' '" 

    #echo "$cmd"
    eval "$cmd"
}

# Return a PU array as if the system was configured
# with the input SMT mode.
# Example:
#  smt=2
#  arr1="0 4 8 12"
#  arr2="0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15"
# create_smt_pus $smt "$arr1" "$arr2" hola
# hola=0 1 4 5 8 9 12 13 
# 
# hash_name: 
#   Associative array with cpus for each topology
#   level, particularly Core and PU (required levels)
create_smt_pus()
{
    local smt=$1 hash_name=$2 
    local core_arr pus_arr i j str pus_idx

    core_arr=( $(eval echo \${$hash_name[Core]}) )
    pus_arr=( $(eval echo \${$hash_name[PU]}) )
    
    pus_idx=0
    for ((i=0; i<${#core_arr[@]}; i++)); do
	for ((j=pus_idx; j<${#pus_arr[@]}; j++)); do
	    if [ ${pus_arr[$j]} -eq ${core_arr[$i]} ]; then
		str+="${pus_arr[@]:$j:$smt} "
		pus_idx=$(( j + smt ))
		break
	    fi
	done
    done
    echo $str
}


# Figure out which numa nodes have gpus and restrict
# the topology to those nodes.
# Assumes global vars:
#   gpus_per_numa
#   arg_ut_range
# Call this function before setting up the restricted
# topology through the utility threads mechanism.
# MPIBIND=vv srun -n4 --mpibind=off -l ./mpibind12-wip /bin/true 
# MPIBIND_UTC_IN=0-5,48-53,18-23,66-71,24-29,72-77,36-41,84-89
restrict_to_numas_wgpus()
{
    declare -A g_pu_array
    local g_topo_levels numa_arr pu_arr num_numas
    local numa_pus numas_wgpus pus_wout_gpus addtl_ut
    local i j tmp
    
    # Get the node's topology for the whole node
    topology "g_topo_levels" "g_pu_array" GT
    #comm_print "gpus_per_numa $(disp_array gpus_per_numa)"

    numa_arr=( ${g_pu_array[NUMANode]} )
    pu_arr=( ${g_pu_array[PU]} )
    num_numas=${#numa_arr[@]}

    get_subsets numa_arr[@] pu_arr[@] "numa_pus"
    #comm_print "numa_pus $(disp_array numa_pus)"

    # Determine the PUs associated with NUMA domains
    # that do not have GPUs so that they can be excluded
    # from the topology through the utility threads mechanism 
    numas_wgpus="${!gpus_per_numa[@]}"
    #comm_print "numas_wgpus $numas_wgpus"
    for ((i=0; i<num_numas; i++)); do
	if [[ ! $numas_wgpus =~ (^|\ )$i(\ |$) ]]; then 
	    pus_wout_gpus+="${numa_pus[$i]}"
	fi
    done

    # Replace spaces with commas
    addtl_ut=${pus_wout_gpus//\ /,}
    if [ -z "$arg_ut_range" ]; then 
	# Chop last comma
	addtl_ut=${addtl_ut%,}
    fi
    #comm_print "pus_wout_gpus $addtl_ut"    

    # Update arg_ut_range to include the PUs we want to
    # exclude (those without GPUs). 
    arg_ut_range=${addtl_ut}${arg_ut_range}
    #comm_print "arg_ut_range $arg_ut_range"

    # ea: continue here 4/27/2019
    # make sure the non-gpu-optimized case works okay too.
    
    # Compact gpus_per_numa since numa domains will be 
    # reassigned based on gpu topology.
    # gpus_per_numa is an associative array (probably
    # it does not need to be--todo).
    # Compact indices
    i=0
    for j in ${!gpus_per_numa[@]}; do
	tmp[$i]="${gpus_per_numa[$j]}"
	i=$(( i+1 ))
    done
    # Overwrite 
    gpus_per_numa=()
    for j in ${!tmp[@]}; do 
	gpus_per_numa[$j]="${tmp[$j]}"
    done
    #comm_print "gpus_per_numa $(disp_array gpus_per_numa)"
}


################################
# Global vars
################################
# Only needed so that emacs parsing of this script remains sane. 
#lshift="<<"
#rshift=">>"

app="$@"
prog=$1
myname=$(basename $0)
topo_cmd=""
topo_file=""
local_rank=""
local_size=""
rank=""
size=""
num_threads=""
num_workers=""
declare -A pu_array pu_size
declare -A gpus_per_numa gpus_per_pu glob_pus_per_gpu 
declare -A nics_per_numa
declare -A mcds_per_numa
declare -a pus_per_gpu worker_to_elems
declare -A env_vars
topo_levels=""
arg_verbose=""
arg_warnings="y"
arg_ignore_vars=""
arg_range=""
arg_ut_range=""
arg_ut_calc=""
arg_ut_pooled=""
arg_ut_dist=""
arg_hbm=""
#arg_smt=""
arg_gpu_optimized=""
arg_sprolog=""
arg_disp_cmd=""
arg_disp_ut=""
arg_debug=""
numa_nodes=""
num_gpus=0 
num_nics=0
num_mcds=0
bindtasks=""
bindmem=""


# Read-in MPI environment 
# Should check this first to setup rank, size, etc. 
mpi_env
#echo "$(hostname) $rank/$size $local_rank/$local_size"

# Parse MPIBIND options
parse_mpibind 

if [ -z "$1" ]; then
    if [ -z "$arg_sprolog" ]; then 
	usage
    fi
fi 

# Look for and set the hwloc command
set_topo_cmd
#comm_print "topo_cmd=$topo_cmd"


# Discover PCI devices
# Fill in:
#   gpus_per_numa, nics_per_numa, 
#   num_gpus, num_nics
discover_devices
#declare -p gpus_per_numa nics_per_numa num_gpus
#declare -p mcds_per_numa
#echo "num_mcds=$num_mcds"


# GPU Optimized
# Figure out which numa nodes have gpus and restrict
# the topology to those nodes.
if [ -n "$arg_gpu_optimized" -a $num_gpus -ge 1 ]; then
    restrict_to_numas_wgpus   
fi


# Calculate the utility threads cpus on a per-process basis. 
#echo "arg=$arg_ut_range"
if [ -n "$arg_ut_range" ]; then 
    # os_cores=0,68,136,204
    # os_cores+=,1,69,137,205
    # os_cores+=,18,86,154,222
    # os_cores+=,19,87,155,223
    #cpu_masks=( $(calc_cpu_masks $os_cores) )
    calc_cpu_masks $arg_ut_range app_cpu_mask ut_cpu_mask
    #comm_print "$ut_cpu_mask"
    #comm_print "$app_cpu_mask"

    # By default assume cpus are pooled
    if [ -n "$arg_ut_dist" ]; then
	# Distribute resources among processes 
	#my_ut_pus=$(map2cpus $ut_cpu_mask)
	map2cpus $ut_cpu_mask my_ut_pus
	
	# Replace spaces with commas
	ut_pus=${my_ut_pus//\ /,}
	# Chop last comma
	ut_pus=${ut_pus%,}
	#printf "%02d: $ut_pus\n" $local_rank

	# Utility libraries can use the indicated resources 
	# through this environment variable. 
	# Resources are process specific 
	#export UTC_PROC_LOCAL=$ut_pus
	UTC_PROC_LOCAL=$ut_pus
	env_vars[UTC_PROC_LOCAL]=$UTC_PROC_LOCAL
    else
	# Nothing to do, same resources for all processes. 
	ut_pus=$arg_ut_range

	# Utility libraries can use the indicated resources 
	# through this environment variable. 
	# Resources are shared between processes - node local 
	#export UTC_NODE_LOCAL=$ut_pus
	env_vars[UTC_NODE_LOCAL]=$ut_pus
    fi
    ut_all_pus=$ut_pus
    #export UTI_CPU_SET=$ut_pus
    UTI_CPU_SET=$ut_pus
    env_vars[UTI_CPU_SET]=$UTI_CPU_SET

    # # Utility libraries can use the indicated resources 
    # # through this environment variable. 
    # export UTC_PROC_LOCAL=$ut_pus
fi

# Get the node's topology 
# If app_cpu_mask is empty, use all of the cpus
topology "topo_levels" "pu_array" AT $app_cpu_mask

# Get the number of elements for each level of the topology
for level in $topo_levels; do 
    pu_size[$level]=$(echo ${pu_array[$level]} | wc -w)
done 


# NUMA domains 
# Number of numa nodes and the 
# PU identifiers where each domain start
numa_nodes=${pu_size[NUMANode]}
numa_array=( ${pu_array[NUMANode]} )
#comm_print "numa_array $(disp_array numa_array)"

# Map tasks to NUMA domains
# Split the processes evenly across NUMA domains 
# to balance memory usage. Memory is more important than compute. 
# 4/30/2019 Todo:
# For architectures with multiple packages and multiple numa
# domains per package/group, I may need to split the tasks
# across packages first and then numa domains within package.
res=( $(map_to_domains $local_rank $local_size $numa_nodes) )
my_numa=${res[0]}
my_np_within_numa=${res[1]}
my_id_within_numa=${res[2]}
#echo "my_numa=$my_numa my_id=$my_id_within_numa my_np=$my_np_within_numa"

# Save the number of tasks per domain in an array
# Used by assign_gpus_to_numadoms
tasks_per_numa=( $(map2doms_elemsperdom $local_size $numa_nodes) )
#comm_print "tasks_per_numa $(disp_array tasks_per_numa)"


# Calculate the number of threads if the user did not provide one.
# Decision is based on having 1 thread/core.
# num_threads gives the number of threads per process to use
# *all* of the cores. If there are more tasks than cores globally 
# or within a NUMA domain, then num_threads=1. But the selected level 
# will be lower than cores (PUs). The select-level piece of code below
# is in charge of this. I need to decide whether I want to pin the app
# processes and threads to all of the PUs at the PU level or cap it 
# at num_threads. Capping it may be a good idea so that we can use the
# remaining threads for system processes or auxiliary threads such as 
# OpenMPI progress thread. 
if [ -z "$OMP_NUM_THREADS" ]; then
    # This version also works but deprecated in favor of 
    # a more general function: map_to_pus
    #nthreads=$(calc_num_threads $my_id_within_numa $my_numa $my_np_within_numa)
    core_array=( ${pu_array[Core]} )
    num_cores=${#core_array[@]}
    if [ $local_size -gt $num_cores ]; then 
	my_nw_within_numa=$my_np_within_numa
	num_threads=1
    else
	get_subsets numa_array[@] core_array[@] "numa_cores"
	my_cores_within_numa="${numa_cores[$my_numa]}"
	res=( $(map_to_pus $my_id_within_numa $my_np_within_numa "$my_cores_within_numa") )

	ncores=$(echo $my_cores_within_numa | wc -w)
	my_nw_within_numa=$ncores
	if [ $my_np_within_numa -gt $ncores ]; then 
	    my_nw_within_numa=$my_np_within_numa
	fi
	num_threads=${#res[@]}
    fi
    #printf "%02d: pus=${res[@]}\n" $local_rank
    
    #export OMP_NUM_THREADS=$num_threads
    env_vars[OMP_NUM_THREADS]=$num_threads
else
    my_nw_within_numa=$(( my_np_within_numa * num_threads ))
fi 

if [[ $arg_debug =~ d ]]; then
    str="%02d: my_numa=$my_numa "
    str+="my_id_wnuma=$my_id_within_numa/$my_np_within_numa "
    str+="num_threads=$num_threads my_nw_wnuma=$my_nw_within_numa"
    printf "$str\n" $local_rank
fi 

# For each domain find out all of its pus at the selected level 
# and save them into an array of strings, e.g., 
# pus of node 1: ${numa_pus[1]}
declare -a numa_pus

# Determine topology level per NUMA
for level in $topo_levels; do 
    numa_pus=()
    level_array=( ${pu_array[$level]} )
    get_subsets numa_array[@] level_array[@] "numa_pus"
    level_size=$(echo ${numa_pus[$my_numa]} | wc -w)
    #declare -p numa_pus
    #printf "%02d: $level: $level_size\n" $local_rank

    # If this level satisfies requirements, don't look any further
    if [ $my_nw_within_numa -le $level_size ]; then
	break 
    fi
done 
if [[ $arg_verbose =~ "vv" ]]; then 
    #comm_print "$myname: Level $level: ${level_array[@]}"
    #comm_print "$myname: Selected level: $level"
    if [ $my_id_within_numa -eq 0 ]; then 
	printf "$myname: %02d: Selected level: $level\n" $local_rank
    fi
fi


# # If the selected level is "higher" than NUMA domains, 
# # e.g., a group that contains two numa domains, 
# # then only use those NUMA domains specified by the
# # selected level.  
# if [ ${#numa_array[@]} -le ${#level_array[@]} ]; then 
#     get_subsets numa_array[@] level_array[@] "numa_pus"
#     # numa_pus_phys is for GPU assignments 
#     numa_pus_phys=("${numa_pus[@]}") 
# else 
#     numa_pus=( ${level_array[@]} )
#     # numa_pus_phys is for GPU assignments 
#     numa_pus_phys=("${numa_array[@]}")
# fi 
# numa_nodes_used=${#numa_pus[@]}




############################################
## Mapping of processes to PUs 
## Assign PUs to tasks 
############################################
my_pus=$(map_to_pus $my_id_within_numa $my_np_within_numa "${numa_pus[$my_numa]}")

# Replace spaces with commas
pus=${my_pus//\ /,}
# Chop last comma
pus=${pus%,}
#printf "%02d: $pus\n" $local_rank


############################################
## Helper or utility threads
############################################
# 9/21/2018
# Need to test MPIBIND_UTC_IN=y. When this is not 
# set the behavior should be as mpibind9-v2, i.e., 
# no utility threads are used. 
# For my utility threads library I should try to 
# assing cpus that are on the same NUMA domain:
# mpibind_pthread_create
# 
# 9/6/2018
# When MPIBIND_UTC_IN is not passed in, mpibind can 
# determine available PUs for utility threads that 
# are not used by the application. 
# Strategy: If selected level is not PU, then find the 
# list of assigned cpus at the PU level (using map_to_pus)
# and take the complement of the selected PUs on the cpus 
# at the PU-level.
# Otherwise, don't do anything because we are already at 
# the level with the max number of cpus. If we wanted to do
# something at this level we could do it through the 
# MPIBIND_UTC_IN environment variable as input to mpibind. 
# For example, we could take the last hardware thread of
# each core. 
# Then export the complement cpus to MPIBIND_UTC_OUT. 

#if [[ -z $arg_ut_range && ! $level =~ PU ]]; then 
if [[ -n $arg_ut_calc && ! $level =~ PU ]]; then 
    numa_pus_all=()
    array=( ${pu_array[PU]} )
    get_subsets numa_array[@] array[@] numa_pus_all
    #echo ${numa_pus_all[$my_numa]} | wc -w
    
    my_pus_all=$(map_to_pus $my_id_within_numa $my_np_within_numa "${numa_pus_all[$my_numa]}")

    #echo "$local_rank my_pus: $my_pus"
    #echo "$local_rank my_pus_all: $my_pus_all"
    ut_pus=$(set_complement "$my_pus" "$my_pus_all" | tr '\n' ',')
    # Chop last comma
    ut_pus=${ut_pus%,}
    #printf "%02d: $ut_pus\n" $local_rank
    
    if [ -n "$ut_all_pus" ]; then 
	ut_all_pus+=,
    fi
    ut_all_pus+=$ut_pus

    if [ -n "$UTC_PROC_LOCAL" ]; then 
	UTC_PROC_LOCAL+=,
    fi
    #export UTC_PROC_LOCAL+=$ut_pus
    UTC_PROC_LOCAL+=$ut_pus 
    env_vars[UTC_PROC_LOCAL]=$UTC_PROC_LOCAL

    if [ -n "$UTI_CPU_SET" ]; then 
	UTI_CPU_SET+=,
    fi
    #export UTI_CPU_SET+=$ut_pus
    UTI_CPU_SET+=$ut_pus
    env_vars[UTI_CPU_SET]+=$UTI_CPU_SET
fi 


############################################
## At least one GPU: Assign GPUs to tasks 
############################################
# Test GPU assignments when there's no physical GPUs: 
if [  ]; then 
    gpus_per_numa=(
	[0]="0 1"
	[1]="2 3 4"
	[2]="5"
	[3]="6 7 8 9"
    )
    num_gpus=10
fi

if [ $num_gpus -ge 1 ]; then 
    # Make sure that each NUMA node has at least a GPU
    if [ ${#gpus_per_numa[@]} -lt $numa_nodes ]; then 
	assign_gpus_to_numadoms
    fi
    if [[ $arg_debug =~ dd ]]; then 
	comm_print "gpus_per_numa: $(disp_array gpus_per_numa)"
    fi

    gpus_per_numa_arr=( ${gpus_per_numa[$my_numa]} )    
    ngpus_within_numa=${#gpus_per_numa_arr[@]}

    ## Assign GPUs to each task. Two cases: 
    ## (1) More tasks than GPUs
    ## (2) More GPUs than tasks (tasks can use more than 1 GPU).  
    if [ $my_np_within_numa -ge $ngpus_within_numa ]; then 
        ## Within a NUMA node, distribute tasks among GPU domains. 
	res=( $(map_to_domains $my_id_within_numa $my_np_within_numa $ngpus_within_numa) )
	my_gpu=${res[0]}
	my_np_within_gpu=${res[1]}
	my_id_within_gpu=${res[2]}

	gpus=${gpus_per_numa_arr[$my_gpu]}
    else
	## Reverse assignment: Assign a task to each GPU
	for ((i=0; i<ngpus_within_numa; i++)); do 
	    res=( $(map_to_domains $i $ngpus_within_numa $my_np_within_numa) )
	    my_task=${res[0]}
	    my_ngpus_within_task=${res[1]}
	    my_id_within_task=${res[2]}

	    if [ $my_id_within_numa -eq $my_task ]; then 
		gpus+="${gpus_per_numa_arr[$i]} "
	    fi 
	done
    fi
    if [[ $arg_debug =~ dd ]]; then 
	printf "%02d: my_numa=$my_numa my_id_wnuma=$my_id_within_numa my_gpu=$gpus my_pu=$my_pus\n" $local_rank 
    fi

    # Chop last space
    gpus=${gpus% }
fi
#printf "%02d: my_pus=$my_pus\n" $local_rank


################################
# MCDRAM affinity
################################

# Currently there's at most one MCDRAM per NUMA. 
# And, there's always one per NUMA when existent. 
if [ $num_mcds -ge 1 ]; then 
    mcds_per_numa_arr=( ${mcds_per_numa[$my_numa]} )
    mcds=${mcds_per_numa_arr[0]}
    #printf "%02d: mcds=$my_mcds\n"
fi


# Todo: work on my pthread intercept call.
# Tested memory binding on snc4-flat and numactl. It works!
# For Sierra, since there may be core specialization I may
# need to check whether one can run on all the cores
# or if there's some cores not allowed for the OS. 
# To show all of the hardware resources, even those where a 
# user cannot run, use the 'whole-system' flag. 
# numactl has a similar parameter (--all or -a). Without it
# numactl may or may not be able to bind to cpus that are 
# isolated with isolcpus. 



# Tests on rzmist: 
#  2 PPN 
#  2 PPN,  1 threads
#  3 PPN,  7 threads
#  3 PPN,  8 threads
#  4 PPN,  7 threads
#  5 PPN,  4 threads
#  5 PPN,  5 threads
#  5 PPN, 25 threads 
#  9 PPN, 17 threads
# 11 PPN,  2 threads
# 23 PPN,  1 threads
# 28 PPN,  1 threads

################################
# NIC affinity
################################

################################
# NVRAM affinity
################################


##################################################
# Finally, bind processes and threads 
##################################################

#echo "threads=$num_threads index=$index"
if [[ $arg_verbose =~ "v" ]]; then
    rank_str=$(printf "rank %2s" $rank)
    cpu_str=$(printf "  cpu $pus")
    if [ -n "$ut_pus" -a -n "$arg_disp_ut" ]; then 
	ut_str=$(printf "  utc $ut_all_pus")
    fi
    if [ $num_gpus -ge 1 ]; then 
	# Determine how much space is needed to printout GPUs
	spc=0
	for i in ${!gpus_per_numa[@]}; do 
	    num=$(echo "${gpus_per_numa[$i]}" | wc -w)
	    if [ $num -gt $spc ]; then 
		spc=$num
	    fi
	done
	spc=$(( 2*spc - 1 )) 
	#echo "spc=$spc"
	gpu_str=$(printf "  gpu %${spc}s" "$gpus")
    fi
    if [ $num_mcds -ge 1 ]; then 
	mcd_str=$(printf "  mcdram %s" $mcds)
    fi 
    # if [ $num_nics -gt 1 ]; then 
    # 	str4=$(printf "%-8s " "nic=")
    # fi
    str="$(hostname)  ${rank_str}${gpu_str}${mcd_str}${cpu_str}${ut_str}${str4}"
    single_node_print "$str"
    #echo "$str"
fi 


# NIC affinity 

# GPU affinity 
if [ $num_gpus -ge 1 ]; then 
    #if [ -z "$CUDA_VISIBLE_DEVICES" ]; then 
    # NVIDIA GPUs 
    #export CUDA_VISIBLE_DEVICES="$gpus"
    env_vars[CUDA_VISIBLE_DEVICES]="$gpus"
    # AMD GPU separator: ','
    #export ROCR_VISIBLE_DEVICES=${gpus//\ /,}
    env_vars[ROCR_VISIBLE_DEVICES]=${gpus//\ /,}
    #else
    #warn_print "GPU affinity defined, disabling mine"
    #fi
fi

# Thread affinity 
if [[ -z $KMP_AFFINITY && -z $OMP_PLACES && \
    -n $(enforce_affinity_var GOMP_CPU_AFFINITY) ]]; then
    pus4omp=$(echo $pus | sed 's/\([0-9]\+\)/{\1}/g') 
    env_vars[OMP_PLACES]=$pus4omp
    #printf "%02d: $pus4omp\n" $local_rank

    # OMP_PROC_BIND values can be true, false, or 
    # a comma separated list of master, close, or spread
    if [[ -z $OMP_PROC_BIND || $OMP_PROC_BIND =~ true ]]; then 
	env_vars[OMP_PROC_BIND]=spread
    fi
else
    warn_print "Thread affinity defined, disabling mine"
fi


# Export environment variables, e.g., GPU, OpenMP affinity. 
export_env_vars


# Memory affinity 
# In KNL flat mode we have two configurations: 
# 1. Default: Use DRAM first
# 2. HBM: Use MCDRAM first 
if [ -n "$arg_hbm" ]; then 
    bindmem="numactl --preferred=$mcds"
fi 

# Process affinity 
# Add utility threads if they were requested
if [ -n "$ut_pus" ]; then
    #ut_pus=,$ut_pus
    ut_all_pus=,$ut_all_pus
fi
# Apparently, I don't need to add the utility cpus 
# to the process affinity. These cpus can still be used
# by threads launched dynamically with explicit affinity. 
#bindtasks="taskset -c $pus$ut_all_pus" 
bindtasks="taskset -c $pus" 


if [ -n "$arg_disp_cmd" ]; then
    single_node_print "$(get_env_vars) $bindtasks $bindmem $@"
fi

if [ -n "$arg_sprolog" ]; then
    taskset -pc $pus $SLURM_TASK_PID
else
    exec $bindtasks $bindmem "$@"
fi
